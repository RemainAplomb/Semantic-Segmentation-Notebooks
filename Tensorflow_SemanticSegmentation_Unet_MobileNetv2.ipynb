{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Info"
      ],
      "metadata": {
        "id": "z5ScT3LDr4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The base dataset link from kaggle: \n",
        "\n",
        "      https://www.kaggle.com/datasets/olekslu/makeup-lips-segmentation-28k-samples"
      ],
      "metadata": {
        "id": "Yvsg_Vc1r7Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method for generating the lips mask dataset:\n",
        "    https://github.com/RemainAplomb/Generate-Dataset/blob/main/Generate_Lips_Mask.ipynb"
      ],
      "metadata": {
        "id": "jtVoDRBdsiTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base code that I employed and used can be found here: \n",
        "    https://www.tensorflow.org/tutorials/images/segmentation"
      ],
      "metadata": {
        "id": "-Z5Pm420sJmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for the easy implementation of the semantic segmentation method provided by Tensorflow.\n",
        "\n",
        "I have modified and explored it so that you will have an easier time training the model.\n",
        "\n",
        "I have provided below multiple specifications for the model that you might want to train. The tflite size and accuracy depends on what specification you will choose."
      ],
      "metadata": {
        "id": "DMG8qvvUsZJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Dependecies"
      ],
      "metadata": {
        "id": "Ncqvh2xGnLVN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwqu3zgDsVOe"
      },
      "outputs": [],
      "source": [
        "# Install kaggle to google colab\n",
        "!pip install -q kaggle\n",
        "!pip install git+https://github.com/tensorflow/examples.git\n",
        "!pip install tensorflow-model-optimization\n",
        "\n",
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import cv2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Dense, Conv1D, Flatten,InputLayer,BatchNormalization\n",
        "from tensorflow.keras.callbacks import ProgbarLogger\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import necessary models\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mount the google drive\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import tensorflow_model_optimization as tfmot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODzzgcbb4Pme"
      },
      "outputs": [],
      "source": [
        "# Upload the kaggle.json\n",
        "# Make kaggle directory\n",
        "!mkdir /kaggle\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!pwd\n",
        "\n",
        "\n",
        "# Download the lips segmentation dataset and\n",
        "# Unzip the it\n",
        "!kaggle datasets download -d remainaplomb/lips-segmentation-dataset-2\n",
        "!unzip /content/lips-segmentation-dataset-2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMP7mglMuGT2"
      },
      "source": [
        "# Our lips segmentation dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilyEFxPCGl8A"
      },
      "outputs": [],
      "source": [
        "# Path of the lips segmentation dataset\n",
        "DATASET_FILEPATH = Path('/content/Lips_Dataset')\n",
        "\n",
        "# Folder for the original image\n",
        "IMAGES = 'original/'\n",
        "\n",
        "# Folder for the mask image\n",
        "MASKS = 'mask/'\n",
        "\n",
        "# Configuration for the resizing of image\n",
        "# and batch size\n",
        "batch_size = 32\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "\n",
        "class_names = [ \"original\", \"mask\" ]\n",
        "\n",
        "# File path for the folders\n",
        "IMAGES_FILEPATH = DATASET_FILEPATH.joinpath(IMAGES)\n",
        "MASKS_FILEPATH = DATASET_FILEPATH.joinpath(MASKS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWUGMOYQAAGN"
      },
      "outputs": [],
      "source": [
        "# Check the differences in both image sets.\n",
        "imgs_set = set(os.listdir(IMAGES_FILEPATH))\n",
        "masks_set = set(os.listdir(MASKS_FILEPATH))\n",
        "\n",
        "imgs_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in imgs_set)\n",
        "masks_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in masks_set)\n",
        "\n",
        "len(imgs_set), len(masks_set)\n",
        "len(imgs_set.difference(masks_set)), len(masks_set.difference(imgs_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiSthJmVAJVr"
      },
      "outputs": [],
      "source": [
        "# Remove image that doesn't have its corresponding mask\n",
        "not_mask = imgs_set.difference(masks_set)\n",
        "\n",
        "not_mask = [f'image{i}.jpg' for i in not_mask]\n",
        "not_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye2EIl2YeLje"
      },
      "source": [
        "# Data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBr6Kq40eQ95",
        "outputId": "7f81382b-24d7-4792-e834-08e104f79583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 5066 - Masks: 5066\n"
          ]
        }
      ],
      "source": [
        "# 1. To load the dataset: image and mask paths\n",
        "# 2. Building the TensorFlow Input Data Pipeline using tf.data API\n",
        "\n",
        "def load_data(path):\n",
        "    images = sorted(glob(os.path.join(path, \"original/*\")))\n",
        "    masks = sorted(glob(os.path.join(path, \"mask/*\")))\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "    x = cv2.resize(x, (256, 256))\n",
        "    x = x / 255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (256, 256))\n",
        "    x[x > 0] = 1\n",
        "    #x = x / 255.0\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def preprocess(x, y):\n",
        "    def f(x, y):\n",
        "        x = x.decode()\n",
        "        y = y.decode()\n",
        "\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    images, masks = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
        "    images.set_shape([256, 256, 3])\n",
        "    masks.set_shape([256, 256, 1])\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "def tf_dataset(x, y):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    #dataset = dataset.shuffle(buffer_size=1000)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    #dataset = dataset.batch(batch)\n",
        "    #dataset = dataset.prefetch(2)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "path = '/content/Lips_Dataset/'\n",
        "\n",
        "images, masks = load_data(path)\n",
        "#images = np.delete( images, np.arange( 15000, 28594, 1))\n",
        "#masks = np.delete( masks, np.arange( 15000, 28540, 1))\n",
        "\n",
        "print(f\"Images: {len(images)} - Masks: {len(masks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndLTW3m6HWJK"
      },
      "outputs": [],
      "source": [
        "for i in range( len(not_mask)):\n",
        "  not_mask[i] = str(IMAGES_FILEPATH.joinpath(not_mask[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "driePo_yLsI_"
      },
      "outputs": [],
      "source": [
        "images = [imagePath for imagePath in images if imagePath not in not_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBN-QtXiU7Iv"
      },
      "outputs": [],
      "source": [
        "dataset = tf_dataset(images, masks)\n",
        "for x, y in dataset:\n",
        "    x = x[0] * 255\n",
        "    y = y[0] * 255\n",
        "\n",
        "    x = x.numpy()\n",
        "    y = y.numpy()\n",
        "\n",
        "    cv2.imwrite(\"image.png\", x)\n",
        "\n",
        "    y = np.squeeze(y, axis=-1)\n",
        "    cv2.imwrite(\"mask.png\", y)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enc8hkZHiJ58"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  #input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask\n",
        "\n",
        "def load_image(original_datapoint, mask_datapoint):\n",
        "  input_image = tf.image.resize(original_datapoint, (256, 256))\n",
        "  input_mask = tf.image.resize(mask_datapoint, (256, 256))\n",
        "\n",
        "  print(input_mask)\n",
        "\n",
        "  #input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwj2-8SaQli"
      },
      "outputs": [],
      "source": [
        "# Specify the parameters for training\n",
        "TRAIN_LENGTH = 5066\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39fYScNz9lmo",
        "outputId": "17cea06a-d8fb-4fe5-9150-a651bae4f55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"resize_1/Squeeze:0\", shape=(256, 256, 1), dtype=float32)\n",
            "Tensor(\"resize_1/Squeeze:0\", shape=(256, 256, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Partition the data into training and testing sets\n",
        "\n",
        "# Take the training dataset and validation dataset\n",
        "val_size = int(TRAIN_LENGTH * 0.2)\n",
        "train_ds = dataset.skip(val_size)\n",
        "val_ds = dataset.take(val_size)\n",
        "\n",
        "train_images = train_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = val_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic-U_17YnFjK",
        "outputId": "28a5b939-06c5-4c31-9fee-3947efd95a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4053\n",
            "1013\n"
          ]
        }
      ],
      "source": [
        "# Check the partitioning of the data\n",
        "print(tf.data.experimental.cardinality(train_images).numpy())\n",
        "print(tf.data.experimental.cardinality(test_images).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJUJEH5oEyB"
      },
      "source": [
        "# Augment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUWdDJRTL0PP"
      },
      "outputs": [],
      "source": [
        "# This is for the augmentation of the data\n",
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "  \n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "# Function for plotting and displaying the original image as well as the mask\n",
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPscskQcNCx4"
      },
      "outputs": [],
      "source": [
        "# Initialize the training batch\n",
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "outputs": [],
      "source": [
        "# Test the training batch\n",
        "for images, masks in train_batches.take(2):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "# Define the model\n",
        "The model being used here is a modified [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). To learn robust features and reduce the number of trainable parameters, use a pretrained model—[MobileNetV2](https://arxiv.org/abs/1801.04381)—as the encoder. For the decoder, you will use the upsample block, which is already implemented in the [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) example in the TensorFlow Examples repo. (Check out the [pix2pix: Image-to-image translation with a conditional GAN](../generative/pix2pix.ipynb) tutorial in a notebook.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "As mentioned, the encoder is a pretrained MobileNetV2 model. You will use the model from `tf.keras.applications`. The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape= [256, 256, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    #'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)"
      ],
      "metadata": {
        "id": "GpnEKR1qM-MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#down_stack = quantize_model(down_stack)\n",
        "down_stack.trainable = False\n",
        "down_stack.summary()"
      ],
      "metadata": {
        "id": "stF2dV9XVRRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [l for l in down_stack.layers]\n",
        "print(len(layers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wp-mzDOxLA2",
        "outputId": "36a606e3-4bcb-41ce-f922-a6806090c930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "outputs": [],
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(128, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(64, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(32, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "metadata": {
        "id": "_6o5iwKlJSeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "outputs": [],
      "source": [
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    \n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "Note that the number of filters on the last layer is set to the number of `output_channels`. This will be one output channel per class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "Now, all that is left to do is to compile and train the model. \n",
        "\n",
        "Since this is a multiclass classification problem, use the `tf.keras.losses.CategoricalCrossentropy` loss function with the `from_logits` argument set to `True`, since the labels are scalar integers instead of vectors of scores for each pixel of every class.\n",
        "\n",
        "When running inference, the label assigned to the pixel is the channel with the highest value. This is what the `create_mask` function is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CLASSES = 3\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6Q7xhMQT4Ugp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVMzbIZLcyEF"
      },
      "source": [
        "Plot the resulting model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw82qF1Gcovr"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "Try out the model to check what it predicts before training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "outputs": [],
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "outputs": [],
      "source": [
        "show_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "The callback defined below is used to observe how the model improves while it is training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "outputs": [],
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHyExYFmrUqF"
      },
      "outputs": [],
      "source": [
        "# Epoch 5 of removed512\n",
        "#checkpoint_path = \"/content/drive/MyDrive/Trained-Models/ConfigTest/Lips.ckpt\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/Trained-Models/1-66mb/Epoch30/Lips.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint( checkpoint_path,\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose= 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd2rlb6F5z-E"
      },
      "outputs": [],
      "source": [
        "model.load_weights( \"/content/drive/MyDrive/Trained-Models/1-66mb/Epoch15/Lips.ckpt\")\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "logdir = tempfile.mkdtemp()"
      ],
      "metadata": {
        "id": "W9oEDNLrE0do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "outputs": [],
      "source": [
        "def cal_steps(num_images, batch_size):\n",
        "   # calculates steps for generator\n",
        "   steps = num_images // batch_size\n",
        "\n",
        "   # adds 1 to the generator steps if the steps multiplied by\n",
        "   # the batch size is less than the total training samples\n",
        "   return steps + 1 if (steps * batch_size) < num_images else steps\n",
        "\n",
        "EPOCHS = 5\n",
        "#VAL_SUBSPLITS = 5\n",
        "#STEPS_PER_EPOCH = cal_steps( 22832, 64)\n",
        "#VALIDATION_STEPS = cal_steps( 5708, 64)\n",
        "\n",
        "STEPS_PER_EPOCH = cal_steps( 4053, 64)\n",
        "VALIDATION_STEPS = cal_steps( 1013, 64)\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[DisplayCallback(), cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Prediction After Training"
      ],
      "metadata": {
        "id": "8gR7sGJrme-D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQVQTbZoXvt-"
      },
      "outputs": [],
      "source": [
        "show_predictions(test_batches, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRqf9ySBbsAt"
      },
      "outputs": [],
      "source": [
        "def get_file_size(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    return size\n",
        "    \n",
        "def convert_bytes(size, unit=None):\n",
        "    if unit == \"KB\":\n",
        "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
        "    elif unit == \"MB\":\n",
        "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
        "    else:\n",
        "        return print('File size: ' + str(size) + ' bytes')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF2 (pb) and Keras (.h5) Export"
      ],
      "metadata": {
        "id": "4c5EGL-EmD6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2VIWs3wPd4T"
      },
      "outputs": [],
      "source": [
        "TF_MODEL_PATH = \"/content/drive/MyDrive/Trained-Models/Epoch30/TF2/Lips_Segmentation_Model_Epoch30_ND_5k_1-66mb\"\n",
        "model.save(TF_MODEL_PATH,save_format='.tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwd2kvYCbzpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b5bbde-8364-4719-d71a-f77b8d4265bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 13.062 Megabytes\n"
          ]
        }
      ],
      "source": [
        "KERAS_MODEL_NAME = \"/content/drive/MyDrive/Trained-Models/Epoch30/Keras/Lips_Segmentation_Model_Epoch30_ND_5k_1-66mb.h5\"\n",
        "model.save(KERAS_MODEL_NAME)\n",
        "convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular Tflite Export"
      ],
      "metadata": {
        "id": "XgOuTfsqmMes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKTdMlptce-e"
      },
      "outputs": [],
      "source": [
        "TF_LITE_MODEL_FILE_NAME = \"/content/drive/MyDrive/Trained-Models/Epoch30/Tflite/Lips_Segmentation_Model_Epoch30_ND_5k.tflite\"\n",
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tf_lite_converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyEaVOggc9ww",
        "outputId": "4ae40a88-7449-47d9-a066-e05ddfe8efd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 25256.395 Kilobytes\n"
          ]
        }
      ],
      "source": [
        "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
        "open(tflite_model_name, \"wb\").write(tflite_model)\n",
        "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the Stats of the Training"
      ],
      "metadata": {
        "id": "j3-Lm7Jvm3nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_for_pruning.evaluate(test_batches, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qehqSAAq6LIn",
        "outputId": "be477f7a-320e-4ac9-fc4c-28f6575114e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 - 179s - loss: 0.0180 - accuracy: 0.9940 - 179s/epoch - 11s/step\n",
            "\n",
            "Test accuracy: 0.9939634799957275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDnkC2tUcWcs",
        "outputId": "6156f9c4-30d1-4677-d70c-4c02342a38a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 - 169s - loss: 0.0045 - accuracy: 0.9982 - 169s/epoch - 11s/step\n",
            "\n",
            "Test accuracy: 0.998233437538147\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_batches, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "outputs": [],
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zza88MZ0i9Pd"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BFE9lvwLfgv"
      },
      "source": [
        "# TF Lite Model (Optimize)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORAx4Yc7LjwM"
      },
      "source": [
        "TF_LITE_MODEL_FILE_NAME = \"/content/drive/MyDrive/Trained-Models/Epoch30/Tflite/Lips_Segmentation_Model_Epoch30_ND_5k_1-66mb.tflite\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY57t7EwCW8P"
      },
      "source": [
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZxnt1IsCXBb"
      },
      "source": [
        "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
        "open(tflite_model_name, \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtIdP296CXFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fecc6c-fbe8-4e9d-8610-1720b226eb01"
      },
      "source": [
        "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 1699.727 Kilobytes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "z5ScT3LDr4Dc",
        "Ncqvh2xGnLVN",
        "sMP7mglMuGT2",
        "ye2EIl2YeLje",
        "Enc8hkZHiJ58",
        "BuJUJEH5oEyB",
        "j0DGH_4T0VYn",
        "1BFE9lvwLfgv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}